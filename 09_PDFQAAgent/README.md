## ğŸ“„ PDFQAAgent â€“ Day 9 of #100DaysOfAI-Agents

A production-quality, NotebookLMâ€‘inspired agent that answers questions strictly from your own sources (PDF, DOCX/TXT, URL, or pasted text). It features a premium UI, session chat history, citations, JSONL logging, and a lightweight onâ€‘disk vector store.

---

### âœ¨ Whatâ€™s Included
- **Modern UI (Tailwind, dark theme)**
  - Intake tabs: Upload PDF, Enter URL, Paste text
  - Realâ€‘time toasts and overlay loaders (uploading, fetching, indexing)
  - Threeâ€‘pane layout: Sources | Chat | Studio (placeholders)
  - Chat shows typing indicator and citations (page + chunk)
  - Session chat history with quick navigation
- **RAG backend**
  - Tokenâ€‘aware chunking with overlap
  - OpenAI embeddings + cosine retrieval (NumPy, onâ€‘disk store)
  - Strict grounding: â€œI donâ€™t knowâ€¦â€ when outâ€‘ofâ€‘context
- **Robust extraction**
  - `pypdf` for PDFs
  - `python-docx` for DOCX
  - `trafilatura` with BeautifulSoup+`lxml` fallback for HTML
- **Observability**
  - JSONL logging per chat turn: question, preview, citations, session_id

---

### ğŸ› ï¸ Tech Stack
- Backend: Flask (Python 3.10+)
- LLM/Embeddings: OpenAI SDK (`gpt-4o-mini`, `text-embedding-3-small` by default)
- Parsing: `pypdf`, `python-docx`, `trafilatura`, `beautifulsoup4`, `lxml`
- Vector store: NumPy + cosine similarity (persisted to disk)
- Config: `python-dotenv`
- UI: Tailwind CSS + vanilla JS

---

### ğŸš€ Quick Start

1) Create venv and install deps
```bash
cd 09_PDFQAAgent
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # macOS/Linux
pip install -r requirements.txt
```

2) Configure environment
Create `.env` in `09_PDFQAAgent/` with:
```env
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small
HOST=127.0.0.1
PORT=8000
DEBUG=true
CHUNK_SIZE_TOKENS=400
CHUNK_OVERLAP_TOKENS=40
TOP_K=6
MAX_CONTEXT_TOKENS=4000
LOG_ENABLED=true
LOG_FILE=logs/pdfqa_log.jsonl
```

3) Run the app
```bash
python server.py
# Open http://127.0.0.1:8000
```

Tip: Use Ctrl+Shift+R for a hard refresh after code changes.

---

### ğŸ’¡ Using the App
- **Upload PDF**: drag & drop or select a file
  - Instant toast confirms reception; overlay indicates extraction
  - On success, youâ€™ll see pages, characters, and file type
- **Enter URL**: paste an article or direct PDF link
  - Toast confirms paste; overlay shows â€œFetching URL and extracting textâ€¦â€
  - Robust extraction with HTML fallback
- **Paste text**: drop raw text
  - Darkâ€‘onâ€‘light input for readability; overlay during indexing
- **Chat**: ask grounded questions
  - Typing indicator while thinking
  - Answers show citations like `p.3#2`
  - Sidebar stores session history (click to reconstruct chat)

---

### ğŸ§© API Endpoints (JSON)
- `POST /api/session` â†’ `{ session_id }`
- `POST /api/upload` (multipart) â†’ `{ doc_id, num_pages, num_chars, ... }`
  - fields: `file`, `session_id`
- `POST /api/fetch_url` â†’ `{ doc_id, num_pages, num_chars, ... }`
  - body: `{ url, session_id }`
- `POST /api/ingest_text` â†’ `{ doc_id, num_pages, num_chars, ... }`
  - body: `{ text, title?, session_id }`
- `POST /api/chat` â†’ `{ answer, citations[], history[] }`
  - body: `{ question, doc_id, session_id }`
- `GET /api/history?session_id=...` â†’ `{ history[] }`

All answers are grounded strictly to retrieved chunks; if not found, the agent responds: â€œI donâ€™t know based on the provided document.â€

---

### ğŸ§  RAG Design
- Chunking: tokenâ€‘aware with configurable overlap (`CHUNK_SIZE_TOKENS`, `CHUNK_OVERLAP_TOKENS`)
- Vector store: `data/docs/<doc_id>/` stores `embeddings.npy`, `chunks.jsonl`, `meta.json`
- Retrieval: cosine similarity over embeddings; `TOP_K` controls context width
- Context budget: `MAX_CONTEXT_TOKENS` ensures prompts stay within model limits
- Citations: page and chunk index are included with each answer

---

### ğŸ“ Logging
- Enabled via `LOG_ENABLED=true`
- Writes JSONL to `LOG_FILE` (default `logs/pdfqa_log.jsonl`)
- Each record: timestamp, session_id, question, answer preview, citations

---

### ğŸ”§ Configuration Summary
| Variable | Default | Notes |
|---|---|---|
| `OPENAI_API_KEY` | â€“ | Required for embeddings and chat |
| `OPENAI_MODEL` | `gpt-4o-mini` | Change as desired |
| `EMBEDDING_MODEL` | `text-embedding-3-small` | Costâ€‘/speedâ€‘optimized |
| `CHUNK_SIZE_TOKENS` | 400 | Token chunk size |
| `CHUNK_OVERLAP_TOKENS` | 40 | Overlap to preserve context |
| `TOP_K` | 6 | Retrieved chunks per query |
| `MAX_CONTEXT_TOKENS` | 4000 | Context cutoff when composing prompt |
| `LOG_ENABLED` | true | Enable JSONL logging |
| `LOG_FILE` | `logs/pdfqa_log.jsonl` | Output path |

---

### ğŸ› Troubleshooting
- **No answers / â€œI donâ€™t knowâ€¦â€**
  - Check `num_chars` in the upload response; if itâ€™s near zero, the source didnâ€™t yield text (try a direct PDF or richer article URL)
  - Verify `OPENAI_API_KEY` and internet connectivity
- **UI still shows â€œsimulatedâ€**
  - Browser cached an old page; use Ctrl+Shift+R or incognito
- **PDF upload succeeds but chat fails**
  - Check Flask console for errors
  - Ensure your key has access to the selected model
- **Windows path issues**
  - Run PowerShell as admin or use cmd; keep the project path ASCIIâ€‘only

---

### ğŸ“¦ Project Structure
```
09_PDFQAAgent/
â”œâ”€â”€ server.py                 # Flask API + session & history
â”œâ”€â”€ config.py                 # .env config & paths
â”œâ”€â”€ openai_service.py         # Embeddings + chat + token counting
â”œâ”€â”€ document_loaders.py       # PDF/DOCX/TXT/URL loaders (robust HTML extraction)
â”œâ”€â”€ text_splitter.py          # Token-aware chunking
â”œâ”€â”€ vector_store.py           # NumPy vector store (cosine)
â”œâ”€â”€ rag_pipeline.py           # Retrieval + prompt assembly + citations
â”œâ”€â”€ logger.py                 # JSONL logging
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # Premium UI (tabs, loaders, chat, history)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/style.css         # Minimal shared styles
â”‚   â””â”€â”€ js/app.js             # (Not used in the inline demo variant)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/              # Uploaded files
â”‚   â””â”€â”€ docs/<doc_id>/        # Embeddings/chunks/meta
â”œâ”€â”€ logs/                     # JSONL logs (if enabled)
â””â”€â”€ requirements.txt
```

---

### ğŸ—ºï¸ Roadmap (optional additions)
- Source panel with page counts and quickâ€‘jump links
- Message actions (copy / expand / quoteâ€‘withâ€‘citation)
- Darkâ€‘mode preference persistence
- SSE/streaming progress for large documents
- Diskâ€‘backed session store (SQLite) for persistence across restarts

---

### ğŸ™ Inspiration
This project is inspired by Google DeepMindâ€™s NotebookLM experience, adapted for a local Flask + OpenAI SDK stack.

---

### ğŸ“„ License
MIT â€” part of the #100DaysOfAI-Agents challenge.
