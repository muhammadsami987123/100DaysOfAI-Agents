import os
import json
from typing import Dict, Any
from config import Config

try:
    import google.generativeai as genai
    HAS_GENAI = True
except Exception:
    genai = None
    HAS_GENAI = False

try:
    from openai import OpenAI
    HAS_OPENAI = True
except Exception:
    OpenAI = None
    HAS_OPENAI = False


class LLMService:
    """Simple LLM wrapper that attempts to use Gemini by default and OpenAI as optional.

    If neither is configured, generate_content will return a fallback mock response.
    """

    def __init__(self):
        self.current_llm = Config.DEFAULT_LLM_MODEL
        self.gemini_client = None
        self.openai_client = None
        self._init_clients()

    def _init_clients(self):
        if HAS_GENAI and Config.GEMINI_API_KEY:
            try:
                genai.configure(api_key=Config.GEMINI_API_KEY)
                self.gemini_client = genai.GenerativeModel("gemini-pro") # Using gemini-pro as a default
            except Exception as e:
                print(f"Failed to initialize Gemini client: {e}")
                self.gemini_client = None
        else:
            if not HAS_GENAI:
                print("google.generativeai package not available; Gemini client disabled")
            else:
                print("GEMINI_API_KEY not set; Gemini client not initialized")

        if HAS_OPENAI and Config.OPENAI_API_KEY:
            try:
                self.openai_client = OpenAI(api_key=Config.OPENAI_API_KEY)
            except Exception as e:
                print(f"Failed to initialize OpenAI client: {e}")
                self.openai_client = None
        else:
            if not HAS_OPENAI:
                print("openai package not available; OpenAI client disabled")
            else:
                print("OPENAI_API_KEY not set; OpenAI client not initialized")

    def set_llm(self, llm_choice: str) -> None:
        if llm_choice in ("gemini", "openai"):
            self.current_llm = llm_choice
        else:
            print(f"Unknown llm choice {llm_choice}, keeping {self.current_llm}")

    def generate_content(self, prompt: str) -> Dict[str, Any]:
        """Try to generate content using the selected LLM; fall back gracefully.

        Returns a dict with keys: summary (str), message (str)
        """
        try:
            if self.current_llm == "gemini" and self.gemini_client:
                response = self.gemini_client.generate_content(prompt)
                text = getattr(response, "text", str(response))
                return {"summary": text, "message": "Content generated by Gemini"}

            if self.current_llm == "openai" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o-mini", # Using gpt-4o-mini as a default
                    messages=[{"role": "user", "content": prompt}],
                )
                text = response.choices[0].message.content
                return {"summary": text, "message": "Content generated by OpenAI"}

            # Fallback: if Gemini requested but not available, try OpenAI client
            if self.current_llm == "gemini" and self.openai_client:
                print("Gemini unavailable, falling back to OpenAI")
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                )
                text = response.choices[0].message.content
                return {"summary": text, "message": "Content generated by OpenAI (fallback)"}

            return {"summary": "", "message": "No LLM configured or available."}

        except Exception as e:
            print(f"Error generating content: {e}")
            return {"summary": "", "message": f"Error generating content: {e}"}
